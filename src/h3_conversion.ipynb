{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2939ef4",
   "metadata": {},
   "source": [
    "# EWRI - Enhanced Wildfire Risk Index Pipeline\n",
    "**Change COUNTY variable below to run for different counties**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "60c8b013",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "County: SUFFOLK | Fire: Pine Barrens Fire (2020)\n"
     ]
    }
   ],
   "source": [
    "# CELL 1: SETTINGS - CHANGE COUNTY HERE\n",
    "COUNTY = \"suffolk\"  # Options: los_angeles, napa, suffolk, maricopa\n",
    "\n",
    "CONFIGS = {\n",
    "    \"los_angeles\": {\"fire_name\": \"Bobcat Fire\", \"fire_year\": 2020, \"state_fips\": \"06\", \"county_fips\": \"037\"},\n",
    "    \"napa\": {\"fire_name\": \"Glass Fire\", \"fire_year\": 2020, \"state_fips\": \"06\", \"county_fips\": \"055\"},\n",
    "    \"suffolk\": {\"fire_name\": \"Pine Barrens Fire\", \"fire_year\": 2020, \"state_fips\": \"36\", \"county_fips\": \"103\"},\n",
    "    \"maricopa\": {\"fire_name\": \"Bush Fire\", \"fire_year\": 2020, \"state_fips\": \"04\", \"county_fips\": \"013\"}\n",
    "}\n",
    "\n",
    "config = CONFIGS[COUNTY]\n",
    "FIRE_NAME, FIRE_YEAR = config[\"fire_name\"], config[\"fire_year\"]\n",
    "STATE_FIPS, COUNTY_FIPS = config[\"state_fips\"], config[\"county_fips\"]\n",
    "\n",
    "BASE = \"/home/network-lab/Desktop/EWRI\"\n",
    "RAW = f\"{BASE}/raw/{COUNTY}\"\n",
    "PROCESSED = f\"{BASE}/processed/{COUNTY}\"\n",
    "OUTPUT = f\"{BASE}/outputs/{COUNTY}\"\n",
    "H3_RES = 9\n",
    "\n",
    "print(f\"County: {COUNTY.upper()} | Fire: {FIRE_NAME} ({FIRE_YEAR})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58dddb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports complete!\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: IMPORTS\n",
    "import pandas as pd, geopandas as gpd, numpy as np, h3, rasterio, os, glob, warnings\n",
    "from shapely.geometry import Point\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tqdm import tqdm\n",
    "warnings.filterwarnings('ignore')\n",
    "os.makedirs(PROCESSED, exist_ok=True)\n",
    "os.makedirs(OUTPUT, exist_ok=True)\n",
    "print(\"Imports complete!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f7b87d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined!\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: HELPER FUNCTIONS\n",
    "def geotiff_to_h3(filepath, res=9):\n",
    "    with rasterio.open(filepath) as src:\n",
    "        data, transform, nodata = src.read(1), src.transform, src.nodata\n",
    "    mask = (data != nodata) if nodata else ~np.isnan(data)\n",
    "    rows, cols = np.where(mask)\n",
    "    results = []\n",
    "    for r, c in zip(rows, cols):\n",
    "        x, y = rasterio.transform.xy(transform, r, c)\n",
    "        results.append({\"h3_index\": h3.latlng_to_cell(y, x, res), \"value\": float(data[r, c])})\n",
    "    return pd.DataFrame(results).groupby(\"h3_index\")[\"value\"].mean().reset_index()\n",
    "\n",
    "def h3_to_point(idx):\n",
    "    lat, lng = h3.cell_to_latlng(idx)\n",
    "    return Point(lng, lat)\n",
    "\n",
    "print(\"Helper functions defined!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59be2d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 31 GeoTIFFs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting: 100%|██████████| 31/31 [56:17<00:00, 108.96s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Satellite: 301,779 hexagons, 31 features\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: CONVERT SATELLITE GeoTIFFs → H3\n",
    "tifs = glob.glob(f\"{RAW}/satellite_data/*.tif\")\n",
    "print(f\"Found {len(tifs)} GeoTIFFs\")\n",
    "\n",
    "sat_dfs = []\n",
    "for tif in tqdm(tifs, desc=\"Converting\"):\n",
    "    name = os.path.basename(tif).replace(\".tif\", \"\")\n",
    "    df = geotiff_to_h3(tif, H3_RES)\n",
    "    df = df.rename(columns={\"value\": name})\n",
    "    sat_dfs.append(df)\n",
    "\n",
    "sat_h3 = sat_dfs[0]\n",
    "for df in sat_dfs[1:]:\n",
    "    sat_h3 = sat_h3.merge(df, on=\"h3_index\", how=\"outer\")\n",
    "\n",
    "sat_h3.to_parquet(f\"{PROCESSED}/satellite_h3.parquet\", index=False)\n",
    "print(f\"Satellite: {len(sat_h3):,} hexagons, {len(sat_h3.columns)-1} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984accd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tracts: 1009\n",
      "FEMA file: NRI_Table_CensusTracts_Arizona.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FEMA→H3: 100%|██████████| 1009/1009 [00:01<00:00, 565.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FEMA: 196,298 hexagons\n"
     ]
    }
   ],
   "source": [
    "# CELL 5: CONVERT FEMA → H3 (SOVI only)\n",
    "tracts = gpd.read_file(glob.glob(f\"{RAW}/census_tracts/*.shp\")[0])\n",
    "tracts = tracts[(tracts[\"STATEFP\"] == STATE_FIPS) & (tracts[\"COUNTYFP\"] == COUNTY_FIPS)].to_crs(\"EPSG:4326\")\n",
    "print(f\"Tracts: {len(tracts)}\")\n",
    "\n",
    "# Fix: Match filename containing \"NRI_Table_CensusTracts\", not just folder path\n",
    "fema_csv = [f for f in glob.glob(f\"{RAW}/fema_data/*/*.csv\") if \"NRI_Table_CensusTracts\" in os.path.basename(f)][0]\n",
    "print(f\"FEMA file: {os.path.basename(fema_csv)}\")\n",
    "fema = pd.read_csv(fema_csv)[[\"TRACTFIPS\", \"SOVI_SCORE\"]]  # SOVI only\n",
    "fema[\"GEOID\"] = fema[\"TRACTFIPS\"].astype(str).str.zfill(11)\n",
    "\n",
    "tracts_fema = tracts.merge(fema, on=\"GEOID\", how=\"inner\")\n",
    "h3_data = []\n",
    "for _, row in tqdm(tracts_fema.iterrows(), total=len(tracts_fema), desc=\"FEMA→H3\"):\n",
    "    try:\n",
    "        for cell in h3.geo_to_cells(row.geometry.__geo_interface__, H3_RES):\n",
    "            h3_data.append({\"h3_index\": cell, \"SOVI_SCORE\": row[\"SOVI_SCORE\"]})\n",
    "    except: pass\n",
    "\n",
    "fema_h3 = pd.DataFrame(h3_data).groupby(\"h3_index\").mean().reset_index()\n",
    "fema_h3.to_parquet(f\"{PROCESSED}/fema_h3.parquet\", index=False)\n",
    "print(f\"FEMA (SOVI): {len(fema_h3):,} hexagons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f950ad66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CONVERTING VULNERABILITY DATA TO H3\n",
      "============================================================\n",
      "\n",
      "1. Processing population data...\n",
      "   Loading: Suffolk_County_Population_2017.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pop 2017: 100%|██████████| 281576/281576 [00:02<00:00, 99363.27it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loading: Suffolk_County_Population_2018.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pop 2018: 100%|██████████| 281576/281576 [00:02<00:00, 100027.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Loading: Suffolk_County_Population_2019.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pop 2019: 100%|██████████| 281576/281576 [00:02<00:00, 99134.95it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Population hexagons: 22,457\n",
      "\n",
      "2. Processing built-up data...\n",
      "   Loading: Suffolk_County_BuiltUp.tif\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BuiltUp: 100%|██████████| 5647015/5647015 [00:54<00:00, 103663.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Building hexagons: 58,569\n",
      "\n",
      "3. Combining population + buildings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Combining: 100%|██████████| 58572/58572 [00:00<00:00, 782771.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "✓ Saved: /home/network-lab/Desktop/EWRI/processed/suffolk/exposure_h3.parquet\n",
      "============================================================\n",
      "  Rows: 58,572\n",
      "  Columns: ['h3_index', 'pop_2017', 'pop_2018', 'pop_2019', 'built_up']\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: CONVERT VULNERABILITY DATA → H3 (Population + Buildings)\n",
    "print(\"=\"*60)\n",
    "print(\"CONVERTING VULNERABILITY DATA TO H3\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "vuln_dir = f\"{RAW}/Vulnerability\"\n",
    "output_path = f\"{PROCESSED}/exposure_h3.parquet\"\n",
    "\n",
    "# County name mapping for file names\n",
    "county_names = {\n",
    "    \"los_angeles\": \"LA_County\",\n",
    "    \"napa\": \"Napa_County\", \n",
    "    \"suffolk\": \"Suffolk_County\",\n",
    "    \"maricopa\": \"Maricopa_County\"\n",
    "}\n",
    "county_prefix = county_names[COUNTY]\n",
    "\n",
    "# 1. POPULATION (WorldPop-style data for 3 years)\n",
    "print(\"\\n1. Processing population data...\")\n",
    "pop_data = defaultdict(lambda: {'pop_2017': 0, 'pop_2018': 0, 'pop_2019': 0})\n",
    "\n",
    "for year in [2017, 2018, 2019]:\n",
    "    tif_path = f\"{vuln_dir}/{county_prefix}_Population_{year}.tif\"\n",
    "    print(f\"   Loading: {os.path.basename(tif_path)}\")\n",
    "    \n",
    "    with rasterio.open(tif_path) as src:\n",
    "        data = src.read(1)\n",
    "        transform = src.transform\n",
    "        nodata = src.nodata\n",
    "        \n",
    "        # Find valid pixels (population > 0)\n",
    "        if nodata is not None:\n",
    "            mask = (data > 0) & (data != nodata)\n",
    "        else:\n",
    "            mask = (data > 0) & ~np.isnan(data)\n",
    "        \n",
    "        rows, cols = np.where(mask)\n",
    "        for r, c in tqdm(zip(rows, cols), total=len(rows), desc=f\"Pop {year}\"):\n",
    "            val = float(data[r, c])\n",
    "            lng, lat = rasterio.transform.xy(transform, r, c)\n",
    "            h3_idx = h3.latlng_to_cell(lat, lng, H3_RES)\n",
    "            pop_data[h3_idx][f'pop_{year}'] += val\n",
    "\n",
    "print(f\"   Population hexagons: {len(pop_data):,}\")\n",
    "\n",
    "# 2. BUILDINGS (BuiltUp raster - sum values per hexagon)\n",
    "print(\"\\n2. Processing built-up data...\")\n",
    "building_data = defaultdict(float)\n",
    "\n",
    "tif_path = f\"{vuln_dir}/{county_prefix}_BuiltUp.tif\"\n",
    "print(f\"   Loading: {os.path.basename(tif_path)}\")\n",
    "\n",
    "with rasterio.open(tif_path) as src:\n",
    "    data = src.read(1)\n",
    "    transform = src.transform\n",
    "    nodata = src.nodata\n",
    "    \n",
    "    if nodata is not None:\n",
    "        mask = (data > 0) & (data != nodata)\n",
    "    else:\n",
    "        mask = (data > 0) & ~np.isnan(data)\n",
    "    \n",
    "    rows, cols = np.where(mask)\n",
    "    for r, c in tqdm(zip(rows, cols), total=len(rows), desc=\"BuiltUp\"):\n",
    "        val = float(data[r, c])\n",
    "        lng, lat = rasterio.transform.xy(transform, r, c)\n",
    "        h3_idx = h3.latlng_to_cell(lat, lng, H3_RES)\n",
    "        building_data[h3_idx] += val\n",
    "\n",
    "print(f\"   Building hexagons: {len(building_data):,}\")\n",
    "\n",
    "# 3. COMBINE INTO ONE DATAFRAME\n",
    "print(\"\\n3. Combining population + buildings...\")\n",
    "all_h3 = set(pop_data.keys()) | set(building_data.keys())\n",
    "\n",
    "records = []\n",
    "for h3_idx in tqdm(all_h3, desc=\"Combining\"):\n",
    "    records.append({\n",
    "        'h3_index': h3_idx,\n",
    "        'pop_2017': pop_data[h3_idx]['pop_2017'],\n",
    "        'pop_2018': pop_data[h3_idx]['pop_2018'],\n",
    "        'pop_2019': pop_data[h3_idx]['pop_2019'],\n",
    "        'built_up': building_data[h3_idx]\n",
    "    })\n",
    "\n",
    "exposure_df = pd.DataFrame(records)\n",
    "exposure_df.to_parquet(output_path, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Saved: {output_path}\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Rows: {len(exposure_df):,}\")\n",
    "print(f\"  Columns: {list(exposure_df.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752bd501",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.10)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
